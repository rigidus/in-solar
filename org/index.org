#+STARTUP: showall indent hidestars
#+SETUPFILE: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup


* Intro

Чтобы облегчить процесс понимания того как это все работает, я написал
эту "краткую, неполную, и (вероятно) местами неверную схему работы", в
надежде, что она пригодиться для онбординга и "связывания концов".

Я "обстучал" эту схему об коллег, и постарался составить ее так, чтобы
можно было понять что представляют собой компоненты и как они
взаимодействуют, читая в порядке изложения. Но в нескольких случаях
приходится "забегать вперед". Я старался минимизировать количество таких
случаев, так что если вы будете изменять этот текст, старайтесь чтобы их
количество не росло.

* Overview

Insolar - это распределенный вычислитель, использующий в качестве системы
хранения ~DLT~ (distributed ledger technology), распределенный реестр.

Распределенный вычислитель состоит из узлов распределенной вычислительной
сети (~Node~), которые обмениваются запросами (~Request~).

~Request~ - это сетевой запрос от одной Node к другой на выполнение
операции над ~Object~-ом.

~Object~ - это экземпляр ~Смарт-контракта~. Например, конкретный кошелек
какого-то пользователя - это ~Object~, у него собственные данные, такие
как количество денег на балансе, которое, разумеется, у каждого кошелька
свое. Но код, который обрабатывает эти данные - общий для всех кошельков
этого типа. Поэтому у всех ~Object~ одного и того же типа есть один
~Class~, в котором хранится этот код. Таким образом ~Class~ в insolar -
это смарт-контракт. Помимио собственно кода в нем могут хранится,
например, миграции между версиями кода, которые изменяют структуру
~Object~-ов. Это позволяет обновлять ~Object~ в момент, когда к нему
происходит первое обращение новой версии кода.

~Node~ - это компьютер, подключенный к сети, который выполняет для сети
какую-то работу в соответствии со своей ~Role~.

~Role~ - это то какую работу ~Node~ делает для сети. Роли бывают
статические и динамические:
- ~Static Role~ - у каждой Node существует одна и только одна статическая
  роль, назначается при регистрации узла. Она используется системой для
  автоматического назначения ~Dynamic Role~, а так же определяет
  некоторые специальные права узлов.
- ~Dynamic Role~ - действует один пульс и определяет права (и
  обязанности) Node по отношению к конкретной сущности (~Object~, ~Jet~ и
  т.п.). У одной Node ~одновременно может быть множество~ ~Dynamic
  Role~-й, в том числе по одной сущности. Однако, некоторые комбинации
  динамических ролей запрещены. Существует две подкатегории ~Dynamic
  role~:
  - ~Allocated Dynamic Role~ - результат автоматического назначения на
    основе ~Static Role~.
  - ~Delegated Dynamic Role~ - результат полной или частичной передачи
    прав по роли от держателя ~Allocated Dynamic Role~. Переделегирование
    не допускается.
  - ~Special Role~ - особые разрешения для Node, не связанные со ~Static
    Role~, но их можно совмещать. Node может иметь ноль и более ролей
    этого типа, они назначаются при регистрации Node.
  - (ещё ожидаются роли Cascade и Encrypt)

Нам интересны некоторые часто используемые динамические роли:
- ~VE~, ~Virtual Executor~ - роль Node, которая может в этом пульсе
  выполнять действия с данным ~Object~.
- ~LME~, ~Light Matherial Executor~ - роль Node, которая в этом пульсе
  производит операции над данными этого ~Object~. Это своего рода кэш
  между ~VE~ и ~HMN~.
- ~HMN~ - постоянное хранилище данных. Пока существует в единственном
  экземпляре.

Этого достаточно чтобы составить себе представление о Roles, необходимое
для дальнейшего объяснения. Более глубокие подробности даны в [[*Node Roles][Node Roles]]

* Pulse

Теперь я начну объяснение с отслеживания пути прохождения ~Request~-а. Но
сначала придется сделать шаг назад и узнать про ~Pulse~.

~Pulse~ - это некоторый временной отрезок, в течении которого набор
~Node~, которые исполняют свои ~Role~, остается постоянным. На каждом
пульсе, для любого выбранной сущности (такой как ~Object~ или ~Jet~)
должна существовать Node с ролью ~VE~, и другая Node с ролью ~LME~.

Когда ~Pulse~ меняется, формируется новый набор узлов, которые будут
выполнять эти роли.

Также ~Pulse~ содержит энтропию, в соответствии с которой в новом пульсе
будут переназначены эти роли - таким образом обеспечивается сменяемость
нод. Этим мы усложняем "сговор" узлов.

Пульс задают ~Pulsar~-ноды. Быть пульсаром - это не одна из ролей, это
совершенно особый тип Node, который занимается только генерацией ~Pulse~.

У нас будет ~Pulsar Network~, который будет использовать распределенный
алгоритм генерации случайного числа, после чего достигают консенсуса в
PBFT-подобном алгоритме и отправляют ~Pulse~ набору случайных нод в
сети. Эти случайные ноды, в свою очередь, распространяют пульс дальше,
своим соседям. В теории, ноды сети не знают где пульсары.

Но в данный момент у нас есть один выделенный пульсар.

Мы заинтереснованы в корректной работе пульсаров, потому что если
очередной пульс не придет - то вся сеть будет продолжать работать в
последнем пульсе, а это не то чего хотелось бы.

Когда ноды получают новый ~Pulse~, они прекращают ту работу, которой
занимались согласно выделенной роли, другую работу. Это называется ~Cмена
пульса~. Например, после смены пульса Node, которая выполняла операции в
предыдущем пульсе может сохранить результаты своей работы.

Теперь когда мы имеем представление о пульсах, можно рассмотреть как
исполняется запрос.

* Request flow

Мы будем, в качестве примера, рассматривать (простой) запрос (например,
запрос о переводе денег от одного пользователя к другому).

Он начинается с (мобильного) приложения пользователя, которое формирует
~Request~ и подписывает его ~Private Key~ пользователя.

[COMMENT:gmm] Я предполагаю, что проверка подписи - затратное мероприятие
по сравнению с формированием запроса, поэтому вредоносные приложения
могут спамить ноды запросами с целью нарушить их работу. Поэтому было бы
правильно уравновесить вычислительные затраты на формирование запроса
чтобы избежать этого вектора атаки. Решение - как fail2ban только
fail2proof, т.е. увеличивать сложность доказательства.

Этот запрос отправляется одной из нод сети и попадат на ~BorderContract~.

- Как приложение узнает о нодах сети и их адресах, куда отправить запрос?
- В данный момент они захардкожены. Но в будущем за это будут отвечать
  ~Discovery Nodes~, о которых известно всем, а они будут предоставлять
  информацию о адресах нужных нод.

~BorderContract~ - это контракт, который может выполнить любая
нода сети, которая может исполнять код.

У ~BorderContract~-ов есть своя внутренняя машинерия, нацеленная на то
чтобы все ноды, которые могут принять API-запрос знали о
~BorderContract~-ах. Она описана в [[*BorderContracts][BorderContracts]] и
https://insolar.atlassian.net/wiki/spaces/DEV/pages/1009287169/Core+v2+-+VN+-+Custom+APIs

API-запрос содержит:
- адрес объекта (в нашем примере - кошелек)
- имя метода (в нашем примере - transfer)
- поля, зависящие от типа запроса
  - amount (кол-во переводимых денег)
  - to (адрес кошелька, кому переводим)
- seed (против replay-attack). Если повторный запрос будет с уже
  известным seed то он будет отвергнут, т.е. на уровне сети нельзя
  послать один запрос дважды.
- подпись пользователя

Задачи ~BorderContract~:
- опеределить ~UserAddr~ - адрес контракта кошелька пользователя. В
  простейшем случае это можно достать из хэш-таблицы вида pubKey:addr
  которая может лежать на ~LM~ и которую мы можем запросить. Ответ будет
  подписан, поэтому ~LM~ теоретически может быть привлечен к
  ответственности за подлог, но [TODO:gmm] - нужно уточнить у Кирилла
  процедуру распознавания подлога.
- создать ~ctx~ - контекст исполнения запроса - структуру данных, которая
  будет сопровождать запрос, и положить туда ~UserAddr~.
- зарегистрировать ~Request~ на ~LME~ и получить подтверждение этого. Это
  гарантирует что запрос будет исполнен.
- Так как теперь известен адрес контракта кошелька пользователя, то можно
  вызвать этот контракт на текущем ~VE~-е для этого контракта. В
  параметрах передаем ~amount~ из запроса:
  ~wallet.transfer(another_wallet, amount)~

- Как определяется текущий ~VE~ для конкретного объекта?
- Любая Нода, зная текущий Пульс и объект, метод которого вызывается,
  может вычислить ~VE~ по известной функции:
  https://insolar.atlassian.net/wiki/spaces/DEV/pages/826212449/2019-06-18+-+06-21+Request+s+Journey+on+the+Insolar+Network

~VE~ вынимает ~UserAddr~ из ~ctx~ и находит соответствующий текущему
пульсу и адресу ~LM~ и получает от него код контракта и его текущее
состояние. Для этого сначала надо взять данные ~Object~-а, а для этого
вычислить ~Jet~, этого ~Object~-а и его местонахождение - это можно
сделать зная ~Jet-Tree~ и список нод ~Light Matherial~, которые на этом
пульсе работают. Возможно ~VE~ имеет неактуальное ~Jet-Tree~, поэтому
когда он пойдет к ~LM~ руководствуясь неактуальным ~Jet-Tree~, тот даст
ему актуальное ~Jet-Tree~.

- Что такое ~Jet~ и ~Jet-Tree~?
- ~Jet~ - набор данных которые представляют состояние ~Object~-а. Каждый
  ~Jet~ хранится на своем ~Light Matherial~ узле, поэтому что бы найти
  нужный ~Jet~ нужно ~Jet-Tree~ - бинарное дерево, по которому можно
  сопоставить идентификатор ~Object~ и ~Node~, которая хранит ~Jet~.

В результате ~VE~ попадает к нужному ~LME~, получает данные и отдельным
запросом код. Но сейчас у нас весь код built-in (вкомпилен во все ноды),
поэтому нам пока нужны только данные.

Если у ~LME~ нет этих данных, он запрашивает их у ~HMN~ или даже у одного
из ~LME~ предыдущих пульсов, получает их и кладет себе в кэш.

Однако ситуация отличается, в случае если один ~VE~ вызывает
другой. Такое может произойти, если один контракт хочет вызвать другой,
хоть это и не наш случай, он важен для понимания, поэтому рассмотрим
его. Тогда вызываемый ~VE~ знает что у вызывающего ~VE~ уже есть все
необходимые данные и новый ~VE~ может (и должен) получить их у
старого. Таким образома система не упирается в скорость обмена с ~LME~.

* Contract execution

Теперь все готово, чтобы начать исполнение контракта.

Теперь начинает работать контракт кошелька пользователя. Для трансфера он
должен:
- зарегистрировать ~incomming request~ на своем ~LME~, получить
  подтверждение регистрации.
- проверить, что пользователь имеет право переводить деньги с кошелька -
  это можно проверить через принадлежность кошелька пользователя
  (например, в объекте кошелька может быть указание на owner-a и тогда мы
  сверяем от чего имени идет исполнение с этим owner-ом)
- проверить что на кошельке хватает баланса
- уменьшить свой баланс на сумму перевода
- вызвать на целевом кошельке метод accept на эту сумму
  (~target.accept(amount)~) Это исходящий SAGA-вызов - он помечен
  аннотацией. Т.к. это исходящий SAGA-вызов (google:"saga pattern") - он
  просто регистрируется на ~LME~ но его исполнение откладывается.
- контракт успешно завершается, на ~LME~ сохраняется его ~Result~ и его
  новое состояние
- в этот момент ~LME~ триггерит запуск SAGA-вызова и исполнение
  продолжается на новом ~VE~ который является текущим в этом пульсе для
  целевого кошелька.
- ~VE~ целевого кошелька сохраняет ~incomming request~ на своем ~LME~ и
  начинает его исполнение

В дальнейшем SAGA может иметь механизм ~rollback~, который заключается в
том, что на исходящей части SAGA-вызова к методу ~accept~ целевого
кошелька будет добавлен метод ~rollback~ (с теми же параметрами что и
~accept~), который будет зарегистрирован на кошельке пользователя, если
accept по какой-то причине не прошел. [TODO:gmm] - Кто должен сделать
регистрацию?

[TODO:gmm] Code Owner = Саша Алексеев. Где этот код?

Завершение исполнения приводит к обновлению данных на ~LME~ целевого
кошелька и мы ([TODO:gmm] - каким образом?) оказываемся снова в
~BorderContract~, который возвращает приложению пользователя информацию о
том что транзакция выполнена (именно транзакция, потому что запрос фактически
был выполнен еще до того как началась обработка SAGA-вызова)

- Как регистрируется запрос на ~Ledger~-е?
- Когда ~VE~ получает входящий запрос, он регистрирует ~incomming
  request~ на своем ~LME~ для этого объекта.
- Когда ~VE~, исполняя контракт, хочет сделать вызов, он сначала на своем
  ~LME~ создает ~outgoing request~, после этого посылает другому ~VE~
  запрос, и этот другой ~VE~ регистрирует на своем ~LME~ ~incomming
  request~. Вызывающий ~VE~ ожидает возврата, таким образом, это
  синхронный вызов. Если какой-то ~LME~ зависнет, то ~VE~ будет ждать на
  этому вызове до окончания пульса. Но остальные объекты будут продолжать
  обрабатываться.
- Существуют еще асинхронные ~NoWait~ вызовы. Они не ожидают возврата, но
  пока таких нет.

- [TODO:gmm] - Retryable Error. Увеличение счетчика попыток для
  запроса. Найти в VN-PROTOCOL в конфле. Там же есть оптимизации для
  скорости не ходим к лайту на каждый чих, а регистрируем ~incomming~ и
  ~outgoing~ в одной пачке, если нет исходящих вызовов.

* Data storage

После окончания пульса ~LME~ делает ~JetDrop~ - т.е. сохраняет данные на
~HMN~ и на некоторое время (light-stash-limit) становится ~LMS~ - ~Light
Matherial Stash~. Это значит что он хранит данные и отдает их в ответ на
запросы. После окончания light-stash-limith он удаляет эти данные и
дальше их можно будет найти только на ~HMN~.

[TODO:gmm] - Какие данные сохраняются?

~Object~. Он состоит из ~Record~-ов, каждый из которых,
представляет его состояние в какой-то временной точке.

~Filament~ - это одна из цепочек внутри ~Object~-а. А объектов в одном
~Jet~-е много. У объекта может быть много филаментов:
- его состояния
- запросов/ответов к нему
- его дочерних объектов

~Edge~ - это еще не закрытые блоки


~Record~ - это некоторый (подписанный создавшей его нодой) набор данных,
адресуемый через свой ~hash~ и номер ~Пульса~. Например ~Request~ и
~Result~ - это ~Record~-ы. Он также может содержать ссылку на другой
~Record~ объединяясь, таким образом, в цепочку. Пример такой цепочки
является ~Object~.

~Virtual Jet~ - это набор афинных ~Object~-ов, т.е. объектов, которые
хранятся (в рамках ~Matherial Jet~) и исполняются (это важнее)
вместе. Т.е. много разных объектов, которые принадлежат к одному ~Jet~-у
(потому что у них id начинается на одну цифру) будут храниться
вместе. ~Virtual Jet~ - это больше про исполнение. Например делегаты
будут всегда исполняться там же где их родитель. Делегат - это что-то
типа объекта-плагина, привязанного к основному объекту. Например, если
существует объект User то его кошелек может быть реализован как
делегат. Тогда, помимо того что он будет исполняться там же где и User,
кошелек можно будет адресовать через родительский объект. Это позволяет
получать объект типа "А" как объект интерфейса "Б". Делегаты пока не
реализованы. [TODO:gmm] - уточнить у Кирилла

~Message~ - это сетевое представление ~Record~-а, состоящего из одного
или более ~Record~-ов. Оно состоит из:
- ~Message Head~ - поднабор полей, который будет передан получателю сразу
  при отправке. Этот поднабор должен влезать в UDP-пакет вместе со
  своей подписью.
- ~Message Content~ - все что не влезло в ~Message Head~.
https://insolar.atlassian.net/wiki/spaces/DEV/pages/1080295571/Core+v2+-+Messages

Несколько ~Message~ могут быть посланы бандлом, т.е. в одной сетевой
посылке.

*** Сплиттинг

Изначально существует один ~Jet~ и все данные пишуться в него. Как только
размер данных на протяжении нескольких пульсов превышает определенный
порог - он разделяется на два, этот процесс называется ~split~. Таким
образом формируется дерево Jet-ов.

Теперь у нас 2 ~Jet~-а, и отсплитившийся ~Jet~ может быть передан другому
~LME~. Если на старый ~LME~ за данными из отсплитившегося ~Jet~-а придет
~VE~, то ему будет сообщено, что он должен переадресовать свой запрос
этому другому ~LME~ и так продолжится рекурсивно, пока данные не будут
найдены.

Обратный процесс - слияние разсплитившихся ~Jet~-ов

[TODO:gmm] Code owner = Романцев. В 2.0 этого когда не будет, потому что там все
переделывается, пока неясно как.

[TODO:gmm] - Высоконагруженные объекты среди слабонагруженных - дописать

[TODO:gmm] Jets, Filaments, Etc - искать в конфле по Романцеву

[TODO:gmm] Консенсус. Работает в паралельном процессе и имеет более
высокий приоритет, чем остальные вещи. Поэтому теперь выделен в соседний
бинарь, чтобы обработка сетевых запросов не могла затормозить
time-critical задачи консенсуса. Предоставляет ActiveList.

[TODO:gmm] Join ноды в сеть:
Изначально предполагалось, что ноды будут сами джойнить друг друга, но
этот механизм оказался сложным в реализации и тестировании, поэтому
решили пока сделать упрощенный алгоритм.
- бутстрап через хэвик - через него все зарегиструются
- дискавери нода подключает хэвик
- механизм бутстрапа будет меняться

[TODO:gmm] - PulseAppender?

Каждый пульс сохраняется на LME как ~JetDrop~, который представляет собой
изменившиеся состояния каждого объекта. Набор состояний конкретного
объекта образует ~LifeLine~.

~Matherial Node~ выполняет хранение данных, так как они подписаны, они
неизменны и иммутабельны, однако [TODO:gmm] - что будет если хранилище не
запишет, посланные ему ~Record~-ы?
- Для этого в 2.0 будем вводить Light-реплики

~Lifeline~ - это последовательность объектных ~Record~-ов.
Подробнее:
https://insolar.atlassian.net/wiki/spaces/DEV/pages/784891920/Data+Model#DataModel-Object

Чтобы отправлять ~Record~ он упаковывается в ~Message~.


[TODO:gmm] - Как разворачивается сеть (Network 101)

** Передача через пульс

Если во время выполнения Request-а меняется пульс, то с момента смены
пульса текущий VE не может продолжать исполнение, потому что в новом
пульсе он уже не является VE для этого объекта. И валидаторы не пропустят
и другие ноды когда проверяют может ли эта нода исполнять запрос увидят
что не может.

Поэтому бывший VE вычисляет новый VE по своему объекту и сообщает новому
VE что продолжает исполнение и ему нужен ~Delegation Token~. Новый ~VE~
проверяет, что в прошлом пульсе старый VE был VE по этому объекту и
выдает этот токен, подписывая его. После чего бывший VE продолжает
исполнение, прикладывая к каждому внешнему вызову этот токен. Получатель
вызова проверяет подпись токена ([TODO:gmm] - откуда он знает открытый
ключ?) и видит что бывший VE может это исполнять.

Если и новый пульс закончился, то бывший VE может опять сходить к еще
более новому VE, приложив свой предыдущий токен и получить свежий
~Delegation Token~. Такое можно повторять до истечения лимита пульсов на
запрос. Если лимит истек, то новый VE отказывает в выдаче токена и
начинает исполнять запрос самостоятельно с самого начала, но
переиспользует сохраненные результаты предудыщих вызовов за счет
"Механизма дедупликации". Это механизм, который регистрирует внешние
вызовы на лайте и если вызов с таким Reason и параметрами (и с тем же
счетчиком) уже был сделан и результат его записан, то не нужно его
повторять.

Когда мы пытаемся дедуплицировать запрос мы можем иметь три состояния
- такого запроса не было - его на до исполнять
- такой запрос был, но результата нет - запрос не регистрируется (так как
  уже зареган), но начинает исполнение
- такой запрос был и есть записанный результат

Этот механизм позволяет сильно ускорить валидацию. В рамках валидации
проверятся, что все вызовы были сделана и проверяются результаты вызовов

Как происходит (будет происходить) валидация:
- Request был зарегистрирован, исполнен, завершен и записан, его можно
  валидировать
- Валидаторы берут его, с темы же значениями и выполняют как бы "в том же
  пульсе" (но на самом деле они работают в следующем пульсе)
- Плюс к этому, валидаторы проверяют всю цепочку вызовов, и их Reason-ы


* Compontents of Platform
** Platform Core
*** Network
**** Node Roles

Виды ролей и связь их с узлами

***** Static Roles

Static Role одна и только одна на узел.

Колонка “гранулярность” в таблице определяет относительно чего
назначаются dynamic roles для узла в соответсвующей статической роли.

| Static Role          | Application Function                                 | Granularity of Dynamic Allocation                                    |
|----------------------+------------------------------------------------------+----------------------------------------------------------------------|
| Neutral (NN)         | Не специфицирована, не выполняющие работы уровня L4+ | Не специфицирована, не выполняющие работы уровня L4+                 |
| Heavy Material (HMN) | Долгосрочное хранине                                 | Material jet segments (набор последовательных drop'ов в одном jet'е) |
| Light Material (LMN) | Запись и краткосрочное хранение drop'ов              | Jet drop'ы / Jet-affined                                             |
| Virtual (VN)         | Вычисления и краткосрочное хранение object'ов        | Object'ы / Object-affined                                            |

****** Dynamic Roles

~Важно~! помнить:

- Динамическая роль назначается конкретному узлу, на конкретный пульс и
  на конкретную сущность (request, object, jet, segment).
- Один узел на один пульс получает множество динамических ролей, в том
  числе и для одной сущности, например: LME(A, P) и LMS(A, P-1).

******* Dynamic Role Targets

Существуют следующие категории сущностей для динамической аллокации (Dynamic Role Targets):

- Virtual jet - обычно упоминается как object / lifeline. Особенность в
  том, что связанные (афинные объекты) не могут исполняться отдельно и
  обозначение VE(A, P) говорит, не только об исполнителе для A, но и для
  всех афинных ему объектов.
- Material jet - обычно упоминается как jet. Это группа храненения для
  virtual jets, сформированная на основе текущего jet tree. Для каждого
  объекта на конкретный пульс всегда однозначно определяется jet.
  - В результате работы LME, по каждому jet'у за пульс будет сформирован
    блок записей - jet drop.
- Material jet segment - последовательность связанных jet drop'ов
  относящихся к одному jet'у (или к его производным после split/merge),
  сохранённая одним HME и хранимая как неделимый набор данных.
  - Хранение в виде сегментов предназначено для упрощение учёта хранения
    и контроля за распространением данных.
  - Requests - запросы. Это обобщающая категория, когда узел не передаёт
    свою динамическую роль, но делегирует часть полномочий на исполнения
    конкретной операции произвольному узлу.

|                                                | Request           | Virtual Jet (object and its affined ones) | Material Jet                    | Material Jet Segment                                                        |
|------------------------------------------------+-------------------+-------------------------------------------+---------------------------------+-----------------------------------------------------------------------------|
| Automatically allocated to                     | NA                | VN                                        | LMN                             | HMN                                                                         |
| Allocation Function                            | NA                | Entropy + ActiveNodes                     | Entropy + ActiveNodes + JetTree | Entropy + ActiveNodes + LME/LMS approval                                    |
| Automatically allocated node ~can delegate to~ | Any ~known*~ node | Any ~active~ node                         | Any ~active~ node               | Any known* HMN                                                              |
| Restrictions                                   | None              | (1)                                       | (1)                             | Аналогично material jet, плюс ограничения по Scattering и Replication rules |
| Dynamic Roles                                  | NA                | (2)                                       | (2)                             | (2)                                                                         |


(1) VE(A, Pn) не может быть VV(A, Pn), т.е. исполнитель не может быть валидатором для своих результатов. См. ниже.
(2) Executor (*E), Replica (*R), Validator(*V), Stash(*S)
*known node - узел за пределами текущего консенсуса, но который можно аутенцифицировать и авторизовать (есть ключ и т.п.)

****** Allocation Restrictions

Возможно два уровня ограничений на совмещение одноим узлом ролей по одной
сущности:

- Обязательный - исполнитель (*E) не может быть автоматически выбран на
  роль валидатора (*V) для проверки собственных результатов. Это
  ограничение действует безусловно, функция выбора валидаторов не долна
  включать узел, вычисленный как *E.
  [NOTE] Это так же должно распространяться и на делегатов, но в
  ограниченной форме. Т.к. делегирование выполняется по запросам, то если
  делегат попал в валидаторы, то необходимо, чтобы для каждого
  делегированного запроса было достаточно валидаторов, не считая
  делегата, его выполнившего.
- Расширенный - если для пульсов P и P-1 исполнителем *E выбран один и
  тот же узел, то для P узел обязан сделать полное делегирование роли на
  следующий (с т.з. allocation function) узел. Данное поведение
  форсируется через *V (откажут в проверке).
  Для VE это так же форсируется через LME (примет только регистрацию
  полной делегации).

Такое поведение необходимо, чтобы функция вычисления исполнителя не
требовала рекурсии по истории аллокаций. Любой другой узел всегда
однозначно вычисляет *E, а в случае такой ситуации - получит от *E
переадресацию на делегата.

****** Special Roles

В настоящий момент используется только одна специальная роль:

- Discovery - узел, используется для подключения к сети узлов, с
  неактуальным списком активных узлов.

В дальнейшем появится роль Dynamic Discovery.

*** BorderContracts

#+NAME:
#+BEGIN_SRC plantuml :file ../img/uml-border-contract.png :eval no-export
  @startuml
  participant C as "RPC Client" order 10
  participant S as "RPC Server" order 20
  participant V as "Virtual Node" order 30
  == Pulse change ==
  S <- : CommitPulse(PN)
  S -> S : SeqN = FirstSeqN = Rand.UInt()\nmapSecrets[PN] = SecretN = Rand.UInt()
  ...
  C -> S : getSeed()
  activate S
  S -> S : seed = [PN, SeqN, hash(PK, SeqN, mapSecrets[PN])]\nmapSeeds[PN][SeqN]=1\nSeqN++
  S -> C : seed
  deactivate S
  note right of S
  To keep seeds:
  - PN, FirstSeqN, SecretN
  - bitmap[# of SeqN]
  => ~13kB per 100k seeds
  end note
  note right of S
  NB! Seed generation & load balancig can be done
  on a separate server. Then the seed should be
  extended by adding [mapSecrets[PN], FirstSeqN]
  encrypted by target's PK.
  end note
  ...
  C -> S : apiCall(payload[seed, endpoint, callSite])
  activate S
  S -> S : assert(mapSeed[seed.PN][seed.SeqN]==1)\nassert(seed.hash == hash(PK, seed.SeqN, mapSecrets[seed.PN])\nmapSeed[seed.PN][seed.SeqN]=0
  V \-> S : facade = getEndpointFacade(endpoint) // cacheable
  S -> S : facade.preValidatePayload(payload)
  V \-> S : slotId := conveyor.AddInput(incomingExternal, payload, validation)
  S -> C : queueId = [PN, slotID, hash(payload)]
  note right of C
  Can also include sign(PK(VN), hash(queueId)) to prevent MitM
  end note
  deactivate S
  activate V
  note right of V
  SM for the request
  end note
  V -> V : identity = validateIdentity(payload) // can call e.g. Kerberos
  V -> : extCallRef = registerExternalIncoming(payload) + \nregisterImpersonatedOutgoing(payload, identity)
  ...
  V ->x V : stop
  deactivate V
  S \-> C : (get a new seed)\nstatus = apiCallStatus(payload[new_seed, queueId])
  note right of S
  apiCallStatus will reject too old PNs, then it goes into 4 ways:
  1. (current PN) will look for an SM by slotID
  2. (past PN) will look for an SM, then in a special cache by slotID
  3. (antique PN) will create a special SM to find a registration record by [PN, hash(payload)]
  end note
  @enduml
#+END_SRC

#+results:
[[file:../img/uml-border-contract.png]]


[[file:../img/uml-border-contract.png]]

** Ledger
** Virtual Machines
** Smart Contracts
** Platform API
** Observer
** Business Foundation
** Application
* Deployment

- [TODO:gmm] - Как развернуть сеть с нуля
- [TODO:gmm] - Как протестировать что-то на развернутой сети
- [TODO:gmm] - Как обновлять
https://insolar.atlassian.net/wiki/spaces/DEV/pages/1038221313/WIP+Insolar+Kubernetes+Deploy+Design

* Testing
* Добавление/удаление нод
* Questioons

- Что такое AppController
  AppController - это функциональность внутри утилиты insolard, которая
  занимается запуском компонентов в нужной конфигурации и выбором схемы
  взаимодействия между этими компонентами.
  https://insolar.atlassian.net/wiki/spaces/DEV/pages/1059979386/Core+v2+-+AppController+design
  AppController нужен для простого и быстрого создания различных
  конфигураций ролей и сети для локального запуска. Цель получить один
  процесс, к которому можно подключиться дебаггером.

- Что такое Scheduler-сервис
  - Узнал отсюда:
    https://insolar.atlassian.net/wiki/spaces/SAIV/pages/1070071873/WIP+Scheduler
- SAIV
- SAGA
  https://microservices.io/patterns/data/saga.html
  [TODO:gmm] - актуальна ли saga для assured ledger?

* Other

Example of GoLang-code

#+NAME:
#+BEGIN_SRC go
    package main

    import (
        "fmt"
        "github.com/pkg/errors"
        "go/ast"
        "go/parser"
        "go/token"
        "io/ioutil"
        "os"
        "reflect"
        "strings"
    )

    const (
        TemplateDirectory = "templates"

        //filename = "src/github.com/insolar/assured-ledger/ledger-core/v2/conveyor/smachine/ping-pong/example/example_3.go"
        filename = "src/github.com/insolar/assured-ledger/ledger-core/v2/logicrunner/sm_object/object.go"
        mainPkg   = "main"
        errorType = "error"
        MachineTypeGoPlugin
    )

    type RecvPair struct {
        Name 	string
        Type 	string
    }
#+END_SRC
